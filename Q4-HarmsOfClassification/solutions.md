# Q4 – Harms of Classification

---

## 1. Representational harm
Representational harm happens when a system reinforces negative stereotypes or unfair associations about a group of people.  
The Kiritchenko & Mohammad (2018) study showed this with sentiment analysis: words describing certain groups (like immigrants, Muslims) were more often linked with negative emotions.  
This demonstrates representational harm because it misrepresents those groups as being more negative than others.

---

## 2. Risk of censorship in toxicity classification
One risk is that comments from minority groups or people discussing sensitive topics might be wrongly flagged as toxic.  
For example, Dixon et al. (2018) and Oliva et al. (2021) showed that sentences with identity terms (like “I am gay”) were sometimes misclassified as offensive, leading to unfair silencing of valid speech.

---

## 3. Why classifiers may perform worse on African American English or Indian English
Classifiers are usually trained on standard American or British English.  
African American English or Indian English use different vocabulary, grammar, or expressions that the model has not seen enough in training.  
Because of this data gap, the classifier may make more mistakes on these varieties, even though they are valid forms of English.
